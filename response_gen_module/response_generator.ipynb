{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d521ab-43f5-4871-ba4a-5e963d1bdcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sp15-chatbot/Documents/backend-chat-processing-subsystem\n",
      "['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/sp15-chatbot/Documents/backend-chat-processing-subsystem/.venv/lib/python3.10/site-packages', '/home/sp15-chatbot/Documents/backend-chat-processing-subsystem/data_management_subsystem']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cur_dir = Path(os.getcwd())\n",
    "par_dir = cur_dir.parent.absolute()\n",
    "sys.path.append(str(par_dir) + \"/data_management_subsystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67fc0db-403f-4e01-87dc-57859c7a77cd",
   "metadata": {
    "id": "f67fc0db-403f-4e01-87dc-57859c7a77cd",
    "outputId": "9211d94d-61aa-466b-a903-bc62e01e9e07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp15-chatbot/Documents/backend-chat-processing-subsystem/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from citation_fetcher import Citation_Fetcher as cf\n",
    "from datetime import datetime as dt\n",
    "from transformers import pipeline\n",
    "from nltk import tokenize as sentence_delim\n",
    "\n",
    "\n",
    "from chat_data_module import ChatData as cd\n",
    "\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332dbfa6-c85f-475e-882a-64c7c2e084e4",
   "metadata": {
    "id": "332dbfa6-c85f-475e-882a-64c7c2e084e4"
   },
   "outputs": [],
   "source": [
    "class ResponseGenerator():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.base_model = None\n",
    "        self.base_system_msg = None\n",
    "        self.kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "\n",
    "    def generate(self, user_input: str, DEBUG_MODE=True):\n",
    "        if DEBUG_MODE:\n",
    "            print(\"Debug mode has been enabled. Capturing runtime...\\n\")\n",
    "            query_gen_start: dt = dt.now()\n",
    "        query_results = None\n",
    "        nltk.download('punkt')\n",
    "        processed_input = sentence_delim.sent_tokenize(user_input)\n",
    "        last_sentence_index = len(processed_input) - 1\n",
    "        processed_input = processed_input[last_sentence_index]\n",
    "        self.kw_extractor = yake.KeywordExtractor()\n",
    "        keywords = self.kw_extractor.extract_keywords(processed_input)\n",
    "\n",
    "        if DEBUG_MODE:\n",
    "            query_gen_end: dt = dt.now()\n",
    "            query_gen_time: dt = query_gen_end - query_gen_start\n",
    "            print(\"Search Query Generation time: \" + str(query_gen_time))\n",
    "            base_model_gen_start: dt = dt.now()\n",
    "\n",
    "        base_model_gen_msg  = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a friendly chatbot that provides reliable information to the user.\n",
    "                    Your goals are to reduce suffering in the universe, increase prosperity in the universe, and increase understanding in the universe.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_input\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        if len(keywords) > 0:\n",
    "            search_query = keywords[0][0]\n",
    "            if DEBUG_MODE:\n",
    "                print(\"Searching for: \\\"\" + search_query + \"\\\"...\")\n",
    "            query_results = cf.search_online(search_query)\n",
    "            base_model_gen_msg.append({\"role\": \"query_results\", \"content\": query_results})\n",
    "            current_time = dt.now()\n",
    "            base_model_gen_msg[0][\"content\"] += (\"You have submitted a query search engine that can help you answer the user's question. \" + \n",
    "                                                 \"Please summarize the query results that can best answer the user's question.\")\n",
    "\n",
    "        self.kw_extractor = None\n",
    "        del self.kw_extractor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.base_model = (pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "            torch_dtype = torch.bfloat16, device_map=\"auto\"))\n",
    "        base_model_prompt = self.base_model.tokenizer.apply_chat_template(base_model_gen_msg, tokenize=False, add_generation_prompt=True)\n",
    "        model_output = self.base_model(base_model_prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "        model_output = model_output[0]\n",
    "        model_output = model_output['generated_text']\n",
    "        model_output = model_output.split(\"<|assistant|>\\n\")\n",
    "        model_output = model_output[1]\n",
    "        self.base_model = None\n",
    "        del self.base_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if DEBUG_MODE:\n",
    "            base_model_gen_end: dt = dt.now()\n",
    "            base_model_gen_time: dt = base_model_gen_end - base_model_gen_start\n",
    "            print(\"Base/Response Model generation time: \" + str(base_model_gen_time))\n",
    "            total_gen_time: dt = query_gen_time + base_model_gen_time\n",
    "            print(\"Total generation time: \" + str(total_gen_time) + \"\\n\")\n",
    "\n",
    "        if query_results is not None:\n",
    "            model_output +=\"\\r\\nSource:\\n\"\n",
    "            i = 0\n",
    "            for item in query_results:\n",
    "                if i >= 3:\n",
    "                    break\n",
    "                else:\n",
    "                    model_output += item['href'] + \"\\n\"\n",
    "                i += 1\n",
    "        \n",
    "        return query_results, model_output\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
