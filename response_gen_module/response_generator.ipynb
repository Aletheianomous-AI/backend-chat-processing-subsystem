{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67fc0db-403f-4e01-87dc-57859c7a77cd",
      "metadata": {
        "id": "f67fc0db-403f-4e01-87dc-57859c7a77cd",
        "outputId": "9211d94d-61aa-466b-a903-bc62e01e9e07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sp15-chatbot/Documents/project_copy/backend-chat-processing-subsystem-dev-david/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from citation_fetcher import Citation_Fetcher as cf\n",
        "from datetime import datetime as dt\n",
        "from transformers import pipeline\n",
        "from nltk import tokenize as sentence_delim\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import torch\n",
        "import yake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332dbfa6-c85f-475e-882a-64c7c2e084e4",
      "metadata": {
        "id": "332dbfa6-c85f-475e-882a-64c7c2e084e4"
      },
      "outputs": [],
      "source": [
        "class ResponseGenerator():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_model = None\n",
        "        self.base_system_msg = None\n",
        "        self.kw_extractor = yake.KeywordExtractor()\n",
        "\n",
        "\n",
        "    def generate(self, user_input: str, DEBUG_MODE=True):\n",
        "        if DEBUG_MODE:\n",
        "            print(\"Debug mode has been enabled. Capturing runtime...\\n\")\n",
        "            query_gen_start: dt = dt.now()\n",
        "        nltk.download('punkt')\n",
        "        processed_input = sentence_delim.sent_tokenize(user_input)\n",
        "        last_sentence_index = len(processed_input) - 1\n",
        "        processed_input = processed_input[last_sentence_index]\n",
        "        keywords = self.kw_extractor.extract_keywords(processed_input)\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            query_gen_end: dt = dt.now()\n",
        "            query_gen_time: dt = query_gen_end - query_gen_start\n",
        "            print(\"Search Query Generation time: \" + str(query_gen_time))\n",
        "            base_model_gen_start: dt = dt.now()\n",
        "\n",
        "        base_model_gen_msg  = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                \"content\": \"\"\"You are a friendly chatbot that provides reliable information to the user.\n",
        "                    Your goals are to reduce suffering in the universe, increase prosperity in the universe, and increase understanding in the universe.\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_input\n",
        "                }\n",
        "            ]\n",
        "        if len(keywords) > 0:\n",
        "            search_query = keywords[0][0]\n",
        "            print(\"Searching for: \\\"\" + search_query + \"\\\"...\")\n",
        "            query_results = cf.search_online(search_query)\n",
        "            base_model_gen_msg.append({\"role\": \"query_results\", \"content\": query_results})\n",
        "            base_model_gen_msg[0][\"content\"].append(\"You have submitted a query search engine that can help you answer the user's question. Please summarize the query results that can best answer the user's question. Cite each result by copying the \\\"href\\\" value.\")\n",
        "        else:\n",
        "            print(\"Skipping search query...\")\n",
        "            pass\n",
        "\n",
        "        self.kw_extractor = None\n",
        "        del self.kw_extractor\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        self.base_model = (pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "            torch_dtype = torch.bfloat16, device_map=\"auto\"))\n",
        "        base_model_prompt = self.base_model.tokenizer.apply_chat_template(base_model_gen_msg, tokenize=False, add_generation_prompt=True)\n",
        "        model_output = self.base_model(base_model_prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "        model_output = model_output.split(\"<|Assistant|>\\n\")\n",
        "        model_output = model_output[1]\n",
        "        self.base_model = None\n",
        "        del self.base_model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            base_model_gen_end: dt = dt.now()\n",
        "            base_model_gen_time: dt = base_model_gen_end - base_model_gen_start\n",
        "            print(\"Base/Response Model generation time: \" + str(base_model_gen_time))\n",
        "            total_gen_time: dt = query_gen_time + base_model_gen_time\n",
        "            print(\"Total generation time: \" + str(total_gen_time) + \"\\n\")\n",
        "        return query_results, model_output\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbae0c7-db0c-434e-880b-f30dfa97f89d",
      "metadata": {
        "id": "6dbae0c7-db0c-434e-880b-f30dfa97f89d",
        "outputId": "8108f713-93d3-4be3-b1b2-821021aee1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug mode has been enabled. Capturing runtime...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/sp15-chatbot/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rg \u001b[38;5;241m=\u001b[39m ResponseGenerator()\n\u001b[0;32m----> 2\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, Alethianomous. What is the weather at Atlanta, Georgia?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEBUG_MODE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mResponseGenerator.generate\u001b[0;34m(self, user_input, DEBUG_MODE)\u001b[0m\n\u001b[1;32m     35\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_extractor\u001b[38;5;241m.\u001b[39mextract_keywords(processed_input)\n\u001b[1;32m     36\u001b[0m top_keyword \u001b[38;5;241m=\u001b[39m keywords[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(top_keyword)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m     40\u001b[0m     query_gen_end: dt \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mnow()\n",
            "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mResponseGenerator.generate\u001b[0;34m(self, user_input, DEBUG_MODE)\u001b[0m\n\u001b[1;32m     35\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_extractor\u001b[38;5;241m.\u001b[39mextract_keywords(processed_input)\n\u001b[1;32m     36\u001b[0m top_keyword \u001b[38;5;241m=\u001b[39m keywords[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(top_keyword)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m     40\u001b[0m     query_gen_end: dt \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mnow()\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/Documents/project_copy/backend-chat-processing-subsystem-dev-david/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/project_copy/backend-chat-processing-subsystem-dev-david/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "rg = ResponseGenerator()\n",
        "query_results, out = rg.generate(\"Hello, Alethianomous. What is the weather at Atlanta, Georgia?\", DEBUG_MODE=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c07d0d-ceb0-4103-aab8-dee65de19786",
      "metadata": {
        "id": "49c07d0d-ceb0-4103-aab8-dee65de19786"
      },
      "outputs": [],
      "source": [
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rg.generate(\"When is OwlCon at Kennesaw State University?\"))"
      ],
      "metadata": {
        "id": "i3IQC06Rt-Tu"
      },
      "id": "i3IQC06Rt-Tu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rg.generate(\"When is the next solar eclipse?\"))"
      ],
      "metadata": {
        "id": "OLwFjbc2uN67"
      },
      "id": "OLwFjbc2uN67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rg.generate(\"Do you think you are sentient?\"))"
      ],
      "metadata": {
        "id": "1BR8oSqIuJ8m"
      },
      "id": "1BR8oSqIuJ8m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prnt(rg.generate(\"How are cars made?\"))"
      ],
      "metadata": {
        "id": "SptWZettuSbS"
      },
      "id": "SptWZettuSbS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}